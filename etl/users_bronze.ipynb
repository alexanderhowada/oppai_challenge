{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b267667d-353a-4e4e-bf2d-b6c6e6bb9d26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from utils.logs import print_args\n",
    "from utils.spark_delta import merge, table_exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b02e7f84-4c0a-41f8-aa72-81b16d5b0a8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class UsersBronzeETL:\n",
    "    def __init__(self, spark, dt_start, dt_end, pk=None):\n",
    "        self.spark = spark\n",
    "        self.dt_start = dt_start\n",
    "        self.dt_end = dt_end\n",
    "        self.pk = self._get_default_pk(pk)\n",
    "        self.default_date = '1901-01-01'\n",
    "        \n",
    "    @staticmethod\n",
    "    def _get_default_pk(pk):\n",
    "        if pk is None:\n",
    "            return ['id_oid']\n",
    "        return pk\n",
    "        \n",
    "    @print_args(print_kwargs=['source_tb'])\n",
    "    def extract(self, source_tb):\n",
    "        latest_update = f\"COALESCE(updated_at_date, created_at_date, TIMESTAMP '{self.default_date}')\"\n",
    "        df = spark.sql(f\"\"\"\n",
    "        WITH latest AS (\n",
    "            SELECT \n",
    "                {','.join(self.pk)}\n",
    "                , MAX({latest_update}) AS latest_update\n",
    "            FROM {source_tb}\n",
    "            WHERE {latest_update} BETWEEN '{self.dt_start}' AND '{self.dt_end}'\n",
    "            GROUP BY {','.join(self.pk)}\n",
    "        )\n",
    "        SELECT s.* FROM {source_tb} s\n",
    "        JOIN latest l\n",
    "            ON {\" AND \".join([f\"s.{c}=l.{c}\" for c in self.pk])}\n",
    "            AND {latest_update}=l.latest_update\n",
    "        WHERE {latest_update} BETWEEN '{self.dt_start}' AND '{self.dt_end}'\n",
    "        \"\"\")\n",
    "        return df\n",
    "    \n",
    "    def transform(self):\n",
    "        pass\n",
    "    \n",
    "    @print_args(print_kwargs=['target_tb', 'drop'])\n",
    "    def load(self, df, target_tb, drop=False):\n",
    "        if drop and table_exists(target_tb, self.spark):\n",
    "            print(f\"Dropping table {target_tb}\")\n",
    "            self.spark.sql(f\"DROP TABLE {target_tb}\").show()\n",
    "\n",
    "        merge(df, target_tb, self.pk, spark_session=self.spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfcc9f16-04df-4fde-a2cc-88d9b5efbe5d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./etl_constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "093d9ccc-fb77-4ec0-952f-22cefb9afb98",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "etl = UsersBronzeETL(spark, DT_START, DT_END)\n",
    "df = etl.extract(source_tb=TARGET_USERS_RAW_TB)\n",
    "etl.load(df, target_tb=TARGET_USERS_BRONZE_TB, drop=DROP_BRONZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "289a4b07-99c7-417f-a843-1e9c16555794",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "807019c5-f8bf-458c-b243-ec33e4418166",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT * FROM {TARGET_USERS_BRONZE_TB}\").count()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "users_bronze",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
